{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms54uWg1gxx9"
      },
      "source": [
        "# Impacts of Optimisation on Asymptotic Convergence of Graph Neural Networks\n",
        "\n",
        "In this Google Colab, we explore the effects of optimisation procedures on the asymptotic convergence of GNNs, as the size of the input graph increases. It is shown in [Adam-Day, 2024](https://arxiv.org/abs/2403.03880), that Graph Neural Networks, when viewed as probabilistic classifiers (i.e. having softmax as the last layer, to give the probability that an input belongs to a class), converge to a fixed vector almost surely, as the size of the input graph tends to infinity. We now look at how this convergence changes under optimisation.\n",
        "\n",
        "The full article is a mini-project for Graph Representation Learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3ZeJCiMDYgJ"
      },
      "source": [
        "# 1) Installing the Relevant Libraries\n",
        "\n",
        "Below we install the relevant libraries and methods, such as PyTorch Geometric, NetworkX, NumPy and MatPlotLib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YCB4k5n6gxYi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HLdg5Z_3hZiX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GPSConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.nn import global_add_pool\n",
        "from scipy.stats import binom\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdumZ3tnJyzn"
      },
      "source": [
        "# 2) Graph Neural Network Models\n",
        "\n",
        "In this section we provide the classes for the GNNs we use. We implement the GCN and GAT architectures. In the paper we do not use GCN, yet we keep it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NnDGDsyAhhnt"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        # self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "        # self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # First GCN layer with ReLU activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        # Second GCN layer with ReLU activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        # # Third GCN layer with ReLU activation\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        # x = F.relu(x)\n",
        "        # Global mean pooling and fully connected layer\n",
        "        x = global_mean_pool(x, batch)  # Aggregate node features\n",
        "        # x = self.fc(x)\n",
        "        return F.softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yeNPNybsadPE"
      },
      "outputs": [],
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        # First GAT layer\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0)\n",
        "        # Second GAT layer\n",
        "        self.gat2 = GATConv(hidden_dim * heads, output_dim, heads=1, dropout=0)\n",
        "        # Fully-connected layer\n",
        "        # self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Apply first GAT layer with ReLU activation\n",
        "        x = F.relu(self.gat1(x, edge_index))\n",
        "        # Apply second GAT layer\n",
        "        x = self.gat2(x, edge_index)\n",
        "        # x = F.relu(x)\n",
        "\n",
        "        # Global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # Aggregate node features\n",
        "        # x = self.fc(x)\n",
        "        return F.softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1Yl_A2KBWq"
      },
      "source": [
        "# 3) Generating Erdős–Rényi Graphs\n",
        "\n",
        "In this section, we implement various classes of Erdős–Rényi graphs, and their conversion to a PyTorch Geometric data type.\n",
        "\n",
        "There are various classification modes we analyse, and it can be picked from the following:\n",
        "- \"parity\" - classifies as 1 if it has an even number of nodes, 0 if odd\n",
        "- \"average_degree\" - classifies as 1 if it has more than the expected number of degrees, 0 otherwise\n",
        "\n",
        "There are also two modes of generating, one where you can specify the probability, and one with inverse probability where the probability is automatically assigned as $\\frac{1}{n}$. The latter provides better training in the average degree classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VFM9BZMnhiFM"
      },
      "outputs": [],
      "source": [
        "def generate_erdos_renyi_graph(num_nodes, probability, feature_dim, mode=\"None\"):\n",
        "    # Generate a random Erdos-Renyi graph using NetworkX\n",
        "    G = nx.erdos_renyi_graph(num_nodes, probability)\n",
        "    # Add random node features\n",
        "    node_features = torch.rand((num_nodes, feature_dim))\n",
        "    # Convert to PyTorch Geometric format\n",
        "    data = from_networkx(G)\n",
        "    data.x = node_features\n",
        "    data.num_nodes = num_nodes\n",
        "\n",
        "    if mode == \"None\":\n",
        "      raise ValueError(\"Mode must be specified\")\n",
        "\n",
        "    if mode == \"parity\":\n",
        "      if num_nodes % 2 == 0:\n",
        "        data.y = torch.tensor([1])\n",
        "      else:\n",
        "        data.y = torch.tensor([0])\n",
        "\n",
        "    if mode == \"average_degree\":\n",
        "      average_degree = torch.tensor([sum(dict(G.degree).values()) / num_nodes])\n",
        "      data.avg_deg = average_degree\n",
        "      if average_degree >= (num_nodes - 1) * probability:\n",
        "        data.y = torch.tensor([1])\n",
        "      else:\n",
        "        data.y = torch.tensor([0])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ercVfcoRBjqh"
      },
      "outputs": [],
      "source": [
        "def generate_erdos_renyi_inverse_prob(num_nodes,  feature_dim, mode=\"None\"):\n",
        "  return generate_erdos_renyi_graph(num_nodes, 1/num_nodes, feature_dim, mode=mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WITJsElU5f7"
      },
      "source": [
        "# 4) Utils\n",
        "\n",
        "This section provides the utils for counting the class memberships, model training, evaluation and plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JS4UPYtV3f3y"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r3T6q8FD3gzD"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            data.to(device)\n",
        "            pred = model(data.x, data.edge_index, data.batch)\n",
        "            correct += (pred.argmax(-1) == data.y).count_nonzero()\n",
        "\n",
        "    return correct / len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wSDPWOmmESa9"
      },
      "outputs": [],
      "source": [
        "def class_frequency(dataloader):\n",
        "\n",
        "  freq = {}\n",
        "\n",
        "  for batch in dataloader:\n",
        "    batch_classes = batch.y\n",
        "    for y in batch_classes:\n",
        "      y = y.item()\n",
        "      if y not in freq:\n",
        "        freq[y] = 1\n",
        "      else:\n",
        "        freq[y] += 1\n",
        "\n",
        "  return freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "BltNtkFIwfHY"
      },
      "outputs": [],
      "source": [
        "def get_softmax_outputs(model, plot_settings, device):\n",
        "\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "\n",
        "    all_graph_sizes = plot_settings['all_graph_sizes']\n",
        "    uncomputed_graph_sizes = plot_settings['uncomputed_graph_sizes']\n",
        "    pre_computed_graph_sizes = plot_settings['pre_computed_graph_sizes']\n",
        "    sample_per_graph = plot_settings['sample_per_graph']\n",
        "    feature_dim = plot_settings['feature_dim']\n",
        "    edge_prob = plot_settings['edge_prob']\n",
        "    mode = plot_settings['mode']\n",
        "    inverse_prob = plot_settings['inverse_prob']\n",
        "\n",
        "    all_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for size in uncomputed_graph_sizes:\n",
        "\n",
        "          size_outputs = []\n",
        "\n",
        "          for _ in range(sample_per_graph):\n",
        "            if inverse_prob:\n",
        "                data = generate_erdos_renyi_inverse_prob(num_nodes=size, feature_dim=feature_dim, mode=mode).to(device)\n",
        "            else:\n",
        "                data = generate_erdos_renyi_graph(num_nodes=size, probability=edge_prob, feature_dim=feature_dim, mode=mode).to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            size_outputs.append(out.cpu().numpy()[0])\n",
        "            # Out gives a list of lists (in case the data is a batch). Hence use out.numpy()[0] to get the first (and only) element.\n",
        "\n",
        "          all_outputs.append(size_outputs)\n",
        "          # averages along the 0th axis (i.e. row average)\n",
        "          average_output = np.average(size_outputs, 0)\n",
        "          outputs.append(average_output)\n",
        "          print(f'Size {size} sample computed.')\n",
        "\n",
        "        for size in pre_computed_graph_sizes:\n",
        "\n",
        "          file_name = 'ER_graphs_10_dim_' + str(size) + '_nodes.pkl'\n",
        "          graphs = load_pickle(file_name)\n",
        "\n",
        "          size_outputs = []\n",
        "\n",
        "          for data in graphs:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            size_outputs.append(out.cpu().numpy()[0])\n",
        "            # Out gives a list of lists (in case the data is a batch). Hence use out.numpy()[0] to get the first (and only) element.\n",
        "\n",
        "\n",
        "          all_outputs.append(size_outputs)\n",
        "          # averages along the 0th axis (i.e. row average)\n",
        "          average_output = np.average(size_outputs, 0)\n",
        "          outputs.append(average_output)\n",
        "          print(f'Size {size} sample computed.')\n",
        "\n",
        "    outputs = np.asarray(outputs)\n",
        "    return all_outputs, outputs\n",
        "\n",
        "\n",
        "def plot_probabilities(all_graph_sizes, outputs):\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for i in range(outputs.shape[1]):\n",
        "      column = outputs[:, i]\n",
        "      plt.plot(all_graph_sizes, column, label=f'Class {i}')\n",
        "\n",
        "    plt.xlabel(\"Epoch\", fontsize=12)\n",
        "    plt.ylabel(\"Class Probabilities\", fontsize=12)\n",
        "    ax = plt.gca()\n",
        "    # ax.set_xscale('log')\n",
        "    ax.set_ylim([0, 1])\n",
        "    plt.yticks(np.arange(0, 1, 0.1))\n",
        "    plt.title(\"Asymptotic Class Probabilities\", fontsize=14)\n",
        "    plt.grid(True)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GHdBSGSYamkl"
      },
      "outputs": [],
      "source": [
        "def dataloader_generator(size_range, number_of_graphs_per_size, feature_dim, split_prob = None, mode=\"None\", prob = None, inverse_prob = False, batch_size=32, shuffle=True):\n",
        "  data = []\n",
        "  for i in range(size_range[0], size_range[1] + 1):\n",
        "    for _ in range(number_of_graphs_per_size):\n",
        "      if inverse_prob:\n",
        "        graph = generate_erdos_renyi_inverse_prob(i, feature_dim, mode)\n",
        "        if split_prob:\n",
        "          binary_label_graphs(graph, split_prob, graph.num_nodes, 1 / graph.num_nodes)\n",
        "        data.append(graph)\n",
        "      if not inverse_prob:\n",
        "        if prob is None:\n",
        "          raise ValueError(\"Cannot have probability as None, if not using the inverse_prob setting!\")\n",
        "          graph = generate_erdos_renyi_inverse_prob(i, prob, feature_dim, mode)\n",
        "          if split_prob:\n",
        "            binary_label_graphs(graph, split_prob, graph.num_nodes, prob)\n",
        "        data.append(graph)\n",
        "\n",
        "  data_loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  return data, data_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binomial_cdf_split(split_prob, n, p):\n",
        "\n",
        "  split = 0\n",
        "\n",
        "  while binom.cdf(split + 1, n, p) < split_prob[0]:\n",
        "    split += 1\n",
        "\n",
        "  if split_prob[0] - binom.cdf(split, n, p) <= binom.cdf(split + 1, n, p) - split_prob[0]:\n",
        "    split1 = split\n",
        "  else:\n",
        "    split1 = split + 1\n",
        "\n",
        "  while binom.cdf(split + 1, n, p) - binom.cdf(split1, n, p) < split_prob[1]:\n",
        "    split += 1\n",
        "\n",
        "  if split_prob[1] - binom.cdf(split, n, p) <= binom.cdf(split + 1, n, p) - split_prob[1]:\n",
        "    split2 = split\n",
        "  else:\n",
        "    split2 = split + 1\n",
        "\n",
        "  return [split1, split2]\n",
        "\n",
        "# s1, s2 = binomial_cdf_split([0.6, 0.3], 10*9, 0.1)\n",
        "# print(binom.cdf(s1, 10*9, 0.1))\n",
        "# print(binom.cdf(s2, 10*9, 0.1))"
      ],
      "metadata": {
        "id": "2oQFgtCAy0e5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zILfjuw84M58"
      },
      "source": [
        "# 5) Pickling Erdős–Rényi Graphs For Fast Computation\n",
        "\n",
        "In this section, we pickle and load the Erdős–Rényi Graphs to compute it quickly during the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra0rWKnw4MiV",
        "outputId": "3b8a3ddb-ad46-4606-f097-15376d10f116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kQtg7TvV4v5-"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save Pickle to the 'gnn_convergence' subfolder\n",
        "def save_pickle(data, file_name):\n",
        "\n",
        "  # Define the file path\n",
        "  file_path = f'./drive/MyDrive/gnn_convergence/{file_name}'\n",
        "\n",
        "  # Save data to the file\n",
        "  with open(file_path, 'wb') as f:\n",
        "      pickle.dump(data, f)\n",
        "\n",
        "  print(f\"File saved to {file_path}\")\n",
        "\n",
        "# Load Pickle from 'gnn_convergence' subfolder\n",
        "def load_pickle(file_name):\n",
        "\n",
        "  # Define the file path\n",
        "  file_path = f'./drive/MyDrive/gnn_convergence/{file_name}'\n",
        "\n",
        "  with open(file_path, 'rb') as f:\n",
        "    loaded_data = pickle.load(f)\n",
        "\n",
        "  return loaded_data\n",
        "\n",
        "def load_graphs_of_size(size):\n",
        "  file_name = 'ER_graphs_10_dim_' + str(size) + '_nodes.pkl'\n",
        "  return load_pickle(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sAP0dep85Xft"
      },
      "outputs": [],
      "source": [
        "# Here, we generate and pickle 20 graphs for each graph size in the ER(n, 1/n) model\n",
        "# The features here are all randomly generated, 10 dimensional features\n",
        "\n",
        "all_graph_sizes = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000, 1200, 1500, 2000, 2200, 2500, 2700, 3000, 3300, 3500, 3700, 4000, 4200, 4400, 4600, 4800, 5000,\n",
        "                   5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 11000, 12000, 13000, 15000, 17000, 20000, 22000, 25000, 27000, 30000, 32000,\n",
        "                   35000, 37000, 40000, 42000, 44000, 46000, 48000, 50000]\n",
        "uncomputed_graph_sizes = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "precomputed_graph_sizes = [100, 200, 300, 400, 500, 700, 1000, 1200, 1500, 2000, 2200, 2500, 2700, 3000, 3300, 3500, 3700, 4000, 4200, 4400, 4600, 4800, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 11000, 12000, 13000, 15000, 17000, 20000, 22000, 25000,\n",
        "                              27000, 30000, 32000, 35000, 37000, 40000, 42000, 44000, 46000, 48000, 50000]\n",
        "\n",
        "graph_per_size = 20\n",
        "feature_dim_samples = 10\n",
        "\n",
        "plot_settings = {\n",
        "    'all_graph_sizes' : all_graph_sizes,\n",
        "    'uncomputed_graph_sizes' : uncomputed_graph_sizes,\n",
        "    'sample_per_graph' : 20,\n",
        "    'pre_computed_graph_sizes' : precomputed_graph_sizes,\n",
        "    'edge_prob' : 0,\n",
        "    'feature_dim' : 10,\n",
        "    'mode' : 'average_degree',\n",
        "    'inverse_prob' : True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to precompute Erdos-Renyi graphs to save time\n",
        "\n",
        "# for size in uncomputed_graph_sizes:\n",
        "#   size_graphs = []\n",
        "#   for _ in range(graph_per_size):\n",
        "#     graph = generate_erdos_renyi_inverse_prob(size, feature_dim_samples, mode='average_degree')\n",
        "#     size_graphs.append(graph)\n",
        "#\n",
        "#   file_name = 'ER_graphs_10_dim_' + str(size) + '_nodes.pkl'\n",
        "#   save_pickle(size_graphs, file_name)\n",
        "#   print(f'Graphs of size {size} is now saved in Google Drive.')"
      ],
      "metadata": {
        "id": "kuwxa7Q02-AF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_label_graphs(graph, split_prob, number_of_nodes, prob):\n",
        "\n",
        "  sp1, sp2 = binomial_cdf_split(split_prob, number_of_nodes * (number_of_nodes - 1), prob)\n",
        "  avg_split_1 = sp1 / number_of_nodes\n",
        "  avg_split_2 = sp2 / number_of_nodes\n",
        "  if graph.avg_deg <= avg_split_1:\n",
        "    graph.y = 0\n",
        "  elif avg_split_1 < graph.avg_deg <= avg_split_2:\n",
        "    graph.y = 1\n",
        "  else:\n",
        "    graph.y = 2"
      ],
      "metadata": {
        "id": "nIAHfOi501lH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader_binary_label_inverse_prob(dataloader, split_prob):\n",
        "\n",
        "  for batch in dataloader:\n",
        "    for graph in batch:\n",
        "      graph_size = graph.num_nodes\n",
        "      binary_label_graphs(graph, split_prob, graph_size, 1 / graph_size)"
      ],
      "metadata": {
        "id": "J4TtE3tz27IA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpvJ_30uVEx5"
      },
      "source": [
        "# 6) Experiments Setup\n",
        "\n",
        "This is the main experiment setup. In this section, we consider $ER(n, \\frac{1}{n})$ graphs and train a GNN to classify whether the average degree falls into certain buckets. Hyperparameters can be changed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Un9WZU073TwS"
      },
      "outputs": [],
      "source": [
        "# Initialising all parameters for the training\n",
        "\n",
        "# set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model & dataset initialisation\n",
        "edge_prob = 0.1\n",
        "split_prob = [0.55, 0.28]\n",
        "feature_dim = 10\n",
        "mode = \"average_degree\"\n",
        "inverse_prob = True\n",
        "\n",
        "# training and test settings\n",
        "train_number_of_graphs_per_size = 50\n",
        "train_range = [10, 100]\n",
        "in_bound_test_number_of_graphs_per_size = 10\n",
        "in_bound_test_range = [10, 100]\n",
        "out_bound_test_number_of_graphs_per_size = 10\n",
        "out_bound_test_range = [500, 600]\n",
        "batch_size = 32\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tSbjzrwoxHdv"
      },
      "outputs": [],
      "source": [
        "# Create synthetic datasets\n",
        "train_data, train_loader = dataloader_generator(size_range=train_range, number_of_graphs_per_size=train_number_of_graphs_per_size, feature_dim=feature_dim, split_prob=split_prob,\n",
        "                                    mode=mode, prob=edge_prob, inverse_prob=inverse_prob, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data, in_bound_test_loader = dataloader_generator(size_range=in_bound_test_range, number_of_graphs_per_size=in_bound_test_number_of_graphs_per_size, feature_dim=feature_dim, split_prob=split_prob,\n",
        "                                            mode=mode, prob=edge_prob, inverse_prob=inverse_prob, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# One can also create out of bound test set as below\n",
        "\n",
        "# out_bound_test_loader = dataloader_generator(size_range=out_bound_test_range, number_of_graphs_per_size=out_bound_test_number_of_graphs_per_size, feature_dim=feature_dim,\n",
        "#                                             mode=mode, prob=edge_prob, inverse_prob=inverse_prob, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ==================\n",
        "# If the train data is already in the computer, can just unpickle it\n",
        "\n",
        "# train_data = load_pickle('train_data_53_20_27.pkl')\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "#\n",
        "# test_data = load_pickle('test_data.pkl')\n",
        "# in_bound_test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ==================\n",
        "\n",
        "# Checks the frequency and proportions of each class\n",
        "print(\"SANITY CHECK\")\n",
        "freq = class_frequency(train_loader)\n",
        "print(freq)\n",
        "print(f'Class 0 Ratio: {freq[0] / (freq[0] + freq[1] + freq[2])}')\n",
        "print(f'Class 1 Ratio: {freq[1] / (freq[0] + freq[1] + freq[2])}')\n",
        "print(f'Class 2 Ratio: {freq[2] / (freq[0] + freq[1] + freq[2])}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train_loop(model, epochs):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    print(f'Train Accuracy: {evaluate(model, train_loader, device)}')\n",
        "    print(f'In-Bound Test Accuracy: {evaluate(model, in_bound_test_loader, device)}')\n",
        "    # print(f'Out-Bound Test Accuracy: {evaluate(model, out_bound_test_loader, device)}')\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "    print(\"=============\")"
      ],
      "metadata": {
        "id": "As_3yXdiP-iw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Varying Epoch Training\n",
        "\n",
        "This is a sample code for training 5 different models with 10 eppchs of training. This code can be reused to make other models. Here we also pickle the data, to be used in plotting later."
      ],
      "metadata": {
        "id": "8hH1wOEGKmHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_count = 5\n",
        "models = [GAT(input_dim=feature_dim, hidden_dim=64, output_dim=3).to(device) for i in range(model_count)]\n",
        "\n",
        "for model in models:\n",
        "  model_train_loop(model, epochs=10)\n",
        "\n",
        "all_outputs_10_epoch, softmax_outputs_10_epochs = [], []\n",
        "\n",
        "for model in models:\n",
        "  all_outputs, outputs = get_softmax_outputs(model, plot_settings, device)\n",
        "  all_outputs_10_epoch.append(all_outputs)\n",
        "  softmax_outputs_10_epochs.append(outputs)\n",
        "\n",
        "save_pickle(softmax_outputs_19_epochs, 'softmax_outputs_10_epochs.pkl')\n",
        "save_pickle(all_outputs_10_epoch, 'all_outputs_10_epochs.pkl')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q9myk5c1Alhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Plotting\n",
        "\n",
        "Below is the code for loading the data, and plotting the class probabilities and standard deviations."
      ],
      "metadata": {
        "id": "N8cEGcFAyDJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_outputs_10_epochs = load_pickle('softmax_outputs_10_epochs.pkl')\n",
        "softmax_outputs_20_epochs = load_pickle('softmax_outputs_20_epochs.pkl')\n",
        "softmax_outputs_50_epochs = load_pickle('softmax_outputs_50_epochs.pkl')\n",
        "softmax_outputs_100_epochs = load_pickle('softmax_outputs_100_epochs.pkl')\n",
        "softmax_outputs_200_epochs = load_pickle('softmax_outputs_200_epochs.pkl')\n",
        "\n",
        "all_outputs_10_epoch = load_pickle('all_outputs_10_epochs.pkl')\n",
        "all_outputs_20_epoch = load_pickle('all_outputs_20_epochs.pkl')\n",
        "all_outputs_50_epoch = load_pickle('all_outputs_50_epochs.pkl')\n",
        "all_outputs_100_epoch = load_pickle('all_outputs_100_epochs.pkl')"
      ],
      "metadata": {
        "id": "KO6TjNs-yE1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASS PROBABILITIES\n",
        "\n",
        "def model_avg(softmax_out):\n",
        "  model_1_output = np.asarray(softmax_out[0])\n",
        "  model_2_output = np.asarray(softmax_out[1])\n",
        "  model_3_output = np.asarray(softmax_out[2])\n",
        "  model_4_output = np.asarray(softmax_out[3])\n",
        "  model_5_output = np.asarray(softmax_out[4])\n",
        "\n",
        "  return (model_1_output + model_2_output + model_3_output + model_4_output + model_5_output) / 5\n",
        "\n",
        "avg_mod_out_0 = model_avg(softmax_outputs_NO_epochs)[:,0]\n",
        "avg_mod_out_10 = model_avg(softmax_outputs_10_epochs)[:,0]\n",
        "avg_mod_out_20 = model_avg(softmax_outputs_20_epochs)[:,0]\n",
        "avg_mod_out_50 = model_avg(softmax_outputs_50_epochs)[:,0]\n",
        "avg_mod_out_100 = model_avg(softmax_outputs_100_epochs)[:,0]\n",
        "\n",
        "x_plot = all_graph_sizes\n",
        "\n",
        "plt.plot(x_plot, avg_mod_out_0, label='No Training', linestyle='-', color='orange')\n",
        "plt.plot(x_plot, avg_mod_out_10, label='10 Epochs', linestyle='-', color='blue')\n",
        "plt.plot(x_plot, avg_mod_out_20, label='20 Epochs', linestyle='-', color='red')\n",
        "plt.plot(x_plot, avg_mod_out_50, label='50 Epochs', linestyle='-', color='green')\n",
        "plt.plot(x_plot, avg_mod_out_100, label='100 Epochs', linestyle='-', color='purple')\n",
        "\n",
        "plt.xlabel('Graph Size')\n",
        "plt.ylabel('Class Probability')\n",
        "plt.yticks(np.arange(0.25, 1, 0.1))\n",
        "plt.title('Top Class Probability with Different Training')\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oBc5cOGNCbTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASS PROBABILITIES\n",
        "\n",
        "model_1_output = np.asarray(softmax_outputs_100_epochs[0])\n",
        "model_2_output = np.asarray(softmax_outputs_100_epochs[1])\n",
        "model_3_output = np.asarray(softmax_outputs_100_epochs[2])\n",
        "model_4_output = np.asarray(softmax_outputs_100_epochs[3])\n",
        "model_5_output = np.asarray(softmax_outputs_100_epochs[4])\n",
        "\n",
        "avg_mod_out = (model_1_output + model_2_output + model_3_output + model_4_output + model_5_output) / 5\n",
        "\n",
        "y_class_1 = avg_mod_out[:,0]\n",
        "y_class_2 = avg_mod_out[:,1]\n",
        "y_class_3 = avg_mod_out[:,2]\n",
        "\n",
        "x_plot = all_graph_sizes\n",
        "\n",
        "plt.plot(x_plot, y_class_1, label='Class 0', linestyle='-', color='green')\n",
        "plt.plot(x_plot, y_class_2, label='Class 1', linestyle='-', color='blue')\n",
        "plt.plot(x_plot, y_class_3, label='Class 2', linestyle='-', color='red')\n",
        "\n",
        "plt.xlabel('Graph Size')\n",
        "plt.ylabel('Class Probabilities')\n",
        "plt.title('Class Probabilities For Increasing Graph Size')\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cv52_luAtyYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STANDARD DEVIATION\n",
        "\n",
        "def process_model(model_outs):\n",
        "\n",
        "  col_1_means = []\n",
        "  col_2_means = []\n",
        "  col_3_means = []\n",
        "\n",
        "  for samples_20 in model_outs:\n",
        "    arr = np.asarray(samples_20)\n",
        "    col_1_means.append(np.mean(arr[:,0]))\n",
        "    col_2_means.append(np.mean(arr[:,1]))\n",
        "    col_3_means.append(np.mean(arr[:,2]))\n",
        "\n",
        "  mean_col_1 = np.mean(col_1_means)\n",
        "  mean_col_2 = np.mean(col_2_means)\n",
        "  mean_col_3 = np.mean(col_3_means)\n",
        "\n",
        "  col_1_sds = []\n",
        "  col_2_sds = []\n",
        "  col_3_sds = []\n",
        "\n",
        "  for samples_20 in model_outs:\n",
        "    arr = np.asarray(samples_20)\n",
        "    std_col_1 = col_1_sds.append(np.std(np.abs(arr[:,0] - mean_col_1)))\n",
        "    std_col_2 = col_2_sds.append(np.std(np.abs(arr[:,1] - mean_col_2)))\n",
        "    std_col_3 = col_3_sds.append(np.std(np.abs(arr[:,2] - mean_col_3)))\n",
        "\n",
        "  return np.array(col_1_sds) + np.array(col_2_sds) + np.array(col_3_sds)\n",
        "\n",
        "def avg_stdev(multiple_model_comp):\n",
        "  stdevs = process_model(multiple_model_comp[0])\n",
        "  for model_outs in multiple_model_comp[1:]:\n",
        "    stdevs += process_model(model_outs)\n",
        "  return stdevs / len(multiple_model_comp)"
      ],
      "metadata": {
        "id": "y-TnTD2cq2UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the y_NO_epoch is not pickled - one needs to generate it\n",
        "y_NO_epoch = avg_stdev(all_outputs_NO_epoch)\n",
        "y_10_epoch = avg_stdev(all_outputs_10_epoch)\n",
        "y_20_epoch = avg_stdev(all_outputs_20_epoch)\n",
        "y_50_epoch = avg_stdev(all_outputs_50_epoch)\n",
        "y_100_epoch = avg_stdev(all_outputs_100_epoch)\n",
        "\n",
        "x_plot = all_graph_sizes\n",
        "\n",
        "plt.plot(x_plot, y_NO_epoch, label='No Training', linestyle='-', color='orange')\n",
        "plt.plot(x_plot, y_10_epoch, label='10 Epochs', linestyle='-', color='blue')  # Add label and style\n",
        "plt.plot(x_plot, y_20_epoch, label='20 Epochs', linestyle='-', color='red')\n",
        "plt.plot(x_plot, y_50_epoch, label='50 Epochs', linestyle='-', color='green')\n",
        "plt.plot(x_plot, y_100_epoch, label='100 Epochs', linestyle='-', color='purple')\n",
        "\n",
        "plt.xlabel('Graph Size')\n",
        "plt.ylabel('Standard Deviation')\n",
        "plt.xscale('log')\n",
        "plt.title('Standard Deviation From Class Mean')\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SMl0PaYzw__C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}